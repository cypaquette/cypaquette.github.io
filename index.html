<!DOCTYPE HTML> <!-- Urban by TEMPLATED templated.co @templatedco Released for free under the Creative Commons Attribution 3.0 license
(templated.co/license) --> <html> <head> <title>Courtney
Paquette</title> <meta charset="utf-8" /> <meta name="viewport"
content="width=device-width, initial-scale=1" /> <link
rel="stylesheet" href="assets/css/main.css" /> </head> <body>


		<!-- Banner --> <section id="banner"> <div
			class="inner"> <header> <h1>Courtney
			Paquette</h1> <p> Assistant Professor,
			Mathematics and Statistics, McGill
			University</p> </header> <a href="#aboutme"
			class="button big scrolly">About Me</a> &nbsp;
			&nbsp; &nbsp; &nbsp; &nbsp; <a
			href="#research" class="button big
			scrolly">Research</a> &nbsp; &nbsp; &nbsp;
			&nbsp; &nbsp; <a href="#presentations"
			class="button big scrolly">Presentations</a> &nbsp; &nbsp; &nbsp;
			&nbsp; &nbsp; <a href="#workshops"
			class="button big scrolly">Workshops &
			Tutorials</a> <p> </p> <a href="#teaching"
			class="button big scrolly">Teaching</a>  &nbsp;
			&nbsp; &nbsp; &nbsp; &nbsp; <a
			href="Research/CV_Paquette.pdf" class="button big
			scrolly">CV</a> &nbsp; &nbsp; &nbsp;
			&nbsp; &nbsp; <a href="#bio"
			class="button big scrolly">Biosketch</a>&nbsp;
			&nbsp; &nbsp; &nbsp; &nbsp; <a
			href="#RMT_OPT_ML_seminar" class="button big
			scrolly">McGill's RMT+ML+OPT seminar</a></div> </section>

<!-- Main --> <div id="main">

<!-- Section --> <section class="wrapper style1" id="aboutme">
<div class="inner"> <!-- 2 Columns-->
<div class="flex flex-2">

<div class="col col1">
<div class="image fit">
  <img src="images/paquette_photo_1.jpg" alt =""/></a>
</div>
		
</div>

<div class="col col2">
  <h3>HELLO!</h3>
  <p> EMAIL: courtney(dot)paquette(at)mcgill(dot)ca <br>
  OFFICE: BURN 913 </p>
<p>
  I am an assistant professor at McGill University in
  the <a href=https://www.mcgill.ca/mathstat/><u>Mathematics and
  Statistics department</u></a>. I am a CIFAR AI Chair (MILA) and I am an
  active member of the Montreal Machine Learning Optimization Group <a
  href=https://mtl-mlopt.github.io/><u>(MTL MLOpt)</u></a> at <a
  href=https://mila.quebec/en/><u>MILA.</u></a>
  Moreover I am one of the lead organizers of the <a
  href=https://opt-ml.org/><u>OPT-ML Workshop</u></a> at NeurIPS since
  2020 and I am one of the lead organizers (and original creator) of
  the <a
  href=https://sites.google.com/view/hidimlearning/home><u>High-dimensional
  Learning Dynamics (HiLD) Workshop</u></a> at ICML. I also work 20%
  as a research scientist at <a
  href=https://ai.google/research/teams/brain/><u>Google DeepMind,
  Montreal.</u></a> You can view my CV <a
  href="Research/CV_Paquette.pdf"><u>here</u></a> if you are
  interested in more details.
  </p>

<p>
  I received my Ph.D. from the Mathematics department at the
  University of Washington (2017) under <a
  href=http://www.math.washington.edu/~ddrusv/><u>Prof. Dmitriy
  Drusvyatskiy</u></a> then I held a postdoctoral position in the
  Industrial and Systems Engineering at Lehigh University where I
  worked with <a
  href=https://coral.ise.lehigh.edu/katyas/><u>Prof. Katya
  Scheinberg</u></a>. I held an NSF postdoctoral fellowship
  (2018-2019) under <a
  href=https://uwaterloo.ca/scholar/vavasis><u>Prof. Stephen
  Vavasis</u></a> in the Combinatorics and Optimization Department at
  the University of Waterloo.
</p>
									  
<p>
  My research broadly focuses on designing and analyzing algorithms
  for large-scale optimization problems, motivated by applications in
  data science. The techniques I use draw from a variety of fields
  including probability, complexity theory, and convex and nonsmooth
  analysis. I also study scaling limits of stochastic algorithms.
</p>
					
<p>
  For a magazine article about myself and my research, see <a
  href=https://cifar.ca/publications-reports/reach/><u>REACH Magazine
  Rising Star in AI, 2022</u></a>. I also recently received the <a
  href=https://sloan.org/><u>Sloan Fellowship in Computer Science (2024)</u></a>.
</p>

<p>
  University of Washington, Lehigh University, University of Waterloo,
  McGill University, and MIlA have strong optimization groups which
  spans across many departments: Math, Stats, CSE, EE, and ISE. If you
  are interested in optimization talks at these places, check out the following seminars:

<ul>
  <li>
  <a href=https://opt-ml.org/https://sites.google.com/view/hidimlearning/home><u>
  High-dimensional Learning Dynamics (HiLD) </u></a>
  workshop at ICML
  </li>

  <li>
  <a href=https://opt-ml.org/><u>Optimization for Machine Learning
  (OPT+ML)</u></a>
  workshop at NeurIPS
  </li>

  <li>
  <a href=https://mtl-mlopt.github.io/><u>Montreal Machine Learning
  and Optimization (MTL MLOPT)</u></a> at MILA
  </li>

  <li>
  <a href=https://dms.umontreal.ca/~mathapp/><u> Applied Mathematics
  </u></a> at McGill University
  </li>

  <li>
  <a href=http://blogs.uw.edu/tops/><u>Trends in Optimization Seminar
  (TOPS/CORE) </u></a> at University of Washington
  </li>

  <li>
  <a href=http://ads-institute.uw.edu/IFDS/about.html><u>Institute for
  Foundations of Data Science</u></a> at University of
  Washington/University of Wisconsin
  </li>

  <li>
  <a href=http://ads-institute.uw.edu/IFDS/about.html><u>Machine
  Learning</u></a> at Paul G. Allen School of Computer Science and
  Engineering, University of Washington
  </li>

  <li>
  <a href=https://coral.ise.lehigh.edu/><u> COR&#64L </u></a> at
  Lehigh University
  </li>
  
  <li>
  <a href=https://uwaterloo.ca/combinatorics-and-optimization/><u>
  Combinatorics and Optimization </u></a> at University of Waterloo
  <ul>
    <li> <a href="Research/U_Waterloo_Seminar_ML.pdf"><u>Machine
    Learning Notes from Fall Term 2018</u></a>
    </li>

    <li> <a href="Research/U_Waterloo_Seminar_HDP.pdf"><u>High
    Dimensional Probability Notes from Spring Term 2019</u></a>
    </li>
  </ul>
  </li>
</ul>
</p>

<p>
  If you are looking for another mathematician (probabilist) named
  Paquette, see <a href="https://people.math.osu.edu/paquette.30/">
<u>Elliot Paquette.</u></a>
</p>
								
</div>
</div>
</div>
</section>

<!-- Section --> <section class="wrapper style2" id="aboutme">
<div class="inner"> <!-- 2 Columns --> 
</div>
</section>

<!-- Section --> <section class="wrapper style1" id="research">
<div class="inner" >
<div class="flex flex-2">
<div class="col col2">

<h2>RESEARCH</h2>
<p>
  My research interests lie at the frontier of large-scale continuous
  optimization. Nonconvexity, nonsmooth analysis, complexity bounds,
  and interactions with random matrix theory and high-dimensional
  statistics appear throughout work. Modern applications of machine
  learning demand these advanced tools and motivate me to develop
  theoretical guarantees with an eye towards immediate practical
  value. My current research program is concerned with developing a
  coherent mathematical framework for analyzing average-case (typical)
  complexity and exact dynamics of learning algorithms in the high-dimensional setting.
</p>

<p>
  You can view my CV <a href="Research/CV_Paquette.pdf"><u>here</u></a> if you
  are interested in more details.
</p>

<p>
  You can view my thesis titled: <a
  href="Research/CP_thesis_v2.pdf"><u>Structure and complexity in
  non-convex and nonsmooth optimization. </u></a>
</p>
					  
<h3>RESEARCH PAPERS </h3>
<p>
  Papers are arranged in reverse chronological order, according to the date they are submitted to the arXiv
</p>

<p>
  * indicates student author
<ul>

<li>Elliot Paquette, Courtney Paquette, Lechao Xiao, Jeffrey
Pennington. <b> <i style="color:#2E8BC0;">4+3 Phases of Compute-Optimal Neural Scaling Laws. </i></b> 
 <i>(submitted),</i> 2024, <a href=https://arxiv.org/pdf/2405.15074><u>https://arxiv.org/pdf/2405.15074</u></a>
					  
</li><p> </p><li>Elizabeth Collins-Woodfin, Inbar Seroussi, Begoña
García Malaxechebarría*, Andrew Mackenzie*, Elliot Paquette, Courtney
Paquette. <b> <i style="color:#2E8BC0;"> The High Line: Exact Risk and
Learning Rate Curves of Stochastic Adaptive Learning Rate
Algorithms. </i> </b> <i>(submitted),</i> 2024, <a
href=><u> arXiv pdf</u></a>

</li><p> </p><li>Tomás González*, Cristóbal Guzman, Courtney
Paquette. <b> <i style="color:#2E8BC0;"> Mirror Descent Algorithms
with Nearly Dimension-Independent Rates for Differentially-Private
Stochastic Saddle-Point Problems. </i> </b> <i>(accepted) 37th Annual
Conference on Learning Theory (COLT),</i> 2024, <a
href=https://arxiv.org/pdf/2403.02912><u>
https://arxiv.org/pdf/2403.02912</u></a>

</li><p> </p><li>Pierre Marion*, Anna Korba, Peter Barlett, Mathieu
Blondel, Valentin De Bortoli, Arnaud Doucet, Felipe Llinares-López*,
Courtney Paquette, Quentin Berthet. <b> <i style="color:#2E8BC0;">
Implicit Diffusion: Efficient Optimization through Stochastic
Sampling. </i> </b> <i>(submitted),</i> 2024, <a
href=https://arxiv.org/pdf/2402.05468><u>
https://arxiv.org/pdf/2402.05468</u></a>

</li><p> </p><li>Elizabeth Collins-Woodfin, Courtney Paquette, Elliot
Paquette, Inbar Seroussi. <b> <i style="color:#2E8BC0;"> Hitting the
High-Dimensional Notes: An ODE for SGD learning dynamics on GLMs and
multi-index models. </i> </b> <i>(submitted),</i> 2023, <a
href=https://arxiv.org/pdf/2308.08977><u>
https://arxiv.org/pdf/2308.08977</u></a>

</li><p> </p><li>Courtney Paquette, Elliot Paquette, Ben Adlam,
Jeffrey Pennington. <b> <i style="color:#2E8BC0;">Implicit
Regularization or Implicit Conditioning? Exact Risk Trajectories of
SGD in High Dimensions. </i></b> <i>Advances in Neural Information
Processing Systems 35 (NeurIPS),</i> 2022, <a
href=https://proceedings.neurips.cc/paper_files/paper/2022/hash/e9d89428e0ef0a70913845b3ae812ee0-Abstract-Conference.html><u>
paper link</u></a>

</li><p> </p><li>Kiwon Lee*, Andrew N. Cheng*, Elliot Paquette,
Courtney Paquette. <b> <i style="color:#2E8BC0;">Trajectory of
Mini-Batch Momentum: Batch Size Saturation and Convergence in
High-Dimensions. </i></b> <i>Advances in Neural Information Processing
Systems 35 (NeurIPS),</i> 2022, <a
href=https://proceedings.neurips.cc/paper_files/paper/2022/hash/efcb76ac1df9231a24893a957fcb9001-Abstract-Conference.html><u>
paper link</u></a>

</li><p> </p><li>Courtney Paquette, Elliot Paquette, Ben Adlam,
Jeffrey Pennington. <b> <i style="color:#2E8BC0;">Homogenization of
SGD in high-dimensions: Exact dynamics and generalization properties. </i></b> <i>(submitted),</i> 2022, <a
href=https://arxiv.org/pdf/2205.07069.pdf><u>
https://arxiv.org/pdf/2205.07069</u></a>

</li><p> </p><li>Leonardo Cunha*, Gauthier Gidel, Fabian Pedregosa,
Damien Scieur, Courtney Paquette. <b> <i style="color:#2E8BC0;">Only
Tails Matter: Average-case Universality and Robustness in the Convex
Regime. </i></b> <i>Proceedings of the 39th International Conference
on Machine Learning (ICML),</i> 2022, <a
href=https://proceedings.mlr.press/v162/cunha22a><u>
https://proceedings.mlr.press/v162/cunha22a</u></a>

</li><p> </p><li>Courtney Paquette, Elliot Paquette. <b> <i
style="color:#2E8BC0;">Dynamics of Stochastic Momentum Methods on
Large-scale, Quadratic Models. </i></b> <i>Advances in Neural
Information Processing Systems 34 (NeurIPS),</i> 2021, <a
href=https://proceedings.neurips.cc/paper/2021/hash/4cf0ed8641cfcbbf46784e620a0316fb-Abstract.html><u>
paper link</u></a>

</li><p> </p><li>Courtney Paquette, Kiwon Lee*, Fabian Pedregosa, Elliot Paquette. <b> <i
style="color:#2E8BC0;">SGD in the Large: Average-case Analysis,
Asymptotics, and Stepsize Criticality. </i></b> <i>34th Annual
Conference on Learning Theory (COLT),</i> 2021, <a
href=https://proceedings.mlr.press/v134/paquette21a><u>
https://proceedings.mlr.press/v134/paquette21a</u></a>

</li><p> </p><li>Courtney Paquette, Bart van Merrienboer, Elliot Paquette, Fabian Pedregosa. <b> <i
style="color:#2E8BC0;">Halting time is predictable for large models: A
Universality Property and Average-case Analysis. </i></b>
<i>Found. Comput. Math. 23 </i> (2023), no.2, 597–673, <a
href=https://doi.org/10.1007/s10208-022-09554-y><u>
https://doi.org/10.1007/s10208-022-09554-y</u></a>

</li><p> </p><li>Sina Baghal, Courtney Paquette, and Stephen A. Vavasis. <b> <i
style="color:#2E8BC0;">A termination criterion for stochastic gradient
for binary classification. </i></b>
<i>(submitted),</i> 2020, <a
href=https://arxiv.org/pdf/2003.10312><u>
https://arxiv.org/pdf/2003.10312</u></a>

</li><p> </p><li>Courtney Paquette, Stephen A. Vavasis. <b> <i
style="color:#2E8BC0;">Potential-based analyses of first-order methods
for constrained and composite optimization. </i></b>
<i>(submitted),</i> 2019, <a
href=https://arxiv.org/pdf/1903.08497><u>
https://arxiv.org/pdf/1903.08497</u></a>

</li><p> </p><li>Courtney Paquette, Katya Scheinberg. <b> <i
style="color:#2E8BC0;">A Stochastic Line Search Method with Expected Complexity Analysis. </i></b>
<i>SIAM J. Optim. 30 </i> (2020), no. 1, 349-376, <a
href=https://doi.org/10.1137/18M1216250><u>
https://doi.org/10.1137/18M1216250</u></a>

</li><p> </p><li>Damek Davis, Dmitriy Drusvyatskiy, Kellie J. MacPhee, Courtney Paquette. <b> <i
style="color:#2E8BC0;">Subgradient methods for sharp weakly convex functions. </i></b>
<i>J. Optim. Theory Appl. (179)</i> (2018), no. 3, 962-982, <a
href=https://doi.org/10.1007/s10957-018-1372-8><u>
https://doi.org/10.1007/s10957-018-1372-8</u></a>

</li><p> </p><li>Damek Davis, Dmitriy Drusvyatskiy, Courtney Paquette. <b> <i
style="color:#2E8BC0;">The nonsmooth landscape of phase retrieval. </i></b>
<i>IMA J. Numer. Anal. 40 </i> (2020), no. 4, 2652-2695, <a
href=https://doi.org/10.1093/imanum/drz031><u>
https://doi.org/10.1093/imanum/drz031</u></a>

</li><p> </p><li>Courtney Paquette, Hongzhou Lin, Dmitriy Drusvyatskiy, Julien Mairal, Zaid Harchaoui. <b> <i
style="color:#2E8BC0;">Catalyst Acceleration for Gradient-Based Non-Convex Optimization. </i></b>
<i>22nd International Conference on Artificial Intelligence and Statistics (AISTATS)</i> 2018, <a
href=http://proceedings.mlr.press/v84/paquette18a.html><u>
http://proceedings.mlr.press/v84/paquette18a.html</u></a>

</li><p> </p><li>Dmitriy Drusvyatskiy, Courtney Paquette. <b> <i
style="color:#2E8BC0;">Efficiency of minimizing compositions of convex functions and smooth maps. </i></b>
<i>Math. Program. 178 </i> (2019), no. 1-2, Ser. A, 503-558, <a
href=https://doi.org/10.1007/s10107-018-1311-3><u>
https://doi.org/10.1007/s10107-018-1311-3</u></a>

</li><p> </p><li>Dmitriy Drusvyatskiy, Courtney Paquette. <b> <i
style="color:#2E8BC0;">Variational analysis of spectral functions simplified. </i></b>
<i>J. Convex Analysis. 25 </i> (2018), no. 1, 119-134, <a
href=https://arxiv.org/pdf/1506.05170><u>
https://arxiv.org/pdf/1506.05170</u></a>

</li>
</ul>
</p>

<h3>EXPOSITORY WRITING </h3>
<p> Survey papers based on research projects. 
</p>
<ul>
</li><p> </p><li>Courtney Paquette, Elliot Paquette. <b> <i
style="color:#2E8BC0;">High-dimensional Optimization. </i></b>
<i>SIAM Views and News 30</i> (2022), 20:16pp, <a
href=https://siagoptimization.github.io/assets/views/ViewsAndNews-30-1.pdf><u>
https://siagoptimization.github.io/assets/views/ViewsAndNews-30-1.pdf</u></a>
</ul>
</div>

<div class="col col1 first">
  <center> Practice meets theory; predicting performance of SGD on
  CIFAR-10 data </center>
  <br>
  
<div class="image fit">
  <img src="images/CIFAR_5M_Emp_Risk_SGD.jpg" alt=""/></a>
</div>

<div class="image fit">
  <img src="images/concentration_sgd_all.jpg" alt=""/></a>
</div>

<center>Exact dynamics of SGD and concentration effects
</center>

<center> <h3> High-dimensional Analysis of Optimization
Algorithms </h3>
</center>

<center>Concentration of halting times
</center>
<br>

<div class="image fit">
  <img src="images/gd-ls.jpg" alt=""/></a>
</div>

<center> <h3> Random Matrix Theory & Machine Learning </h3>
</center>

<center> Eigenvalues of covariance matrix of MNIST data set using random features
</center>
<br>

<div class="image fit">
  <img src="images/MNIST_eigenvalues.jpg" alt=""/></a>
</div>

<center> <h3>Stochastic Optimization</h3>
</center>

<center>SGD + momentum parameter choices 
</center>
<br>

<div class="image fit">
  <img src="images/heatmap_malthusian_main.jpg" alt=""/></a>
</div>

<center> <h3>Nonsmooth & Nonconvex</h3>
</center>

<center>Convex composite (nonsmooth,nonconvex) of robust phase retrieval
</center>
<br>

<div class="image fit">
  <img src="images/graph_true_func.jpg" alt=""/></a>
</div>

</div>
</div>
</div>
</section>

<!-- Section --> <section class="wrapper style2" id="students">
<div class="inner"> <!-- 2 Columns-->
<div class="flex flex-2"> <div class="col col2">
  <h2>STUDENTS</h2>
  <h3>MSc Students</h3>
  <ul>
    <li>  <font color = #E9DCC9>Matt Chaubet,</font color> 2024-present
    </li>

    <li>  <font color = #E9DCC9>Yixi Wang,</font color> 2024-present
    </li>

    <li>  <font color = #E9DCC9>Andrew Mackenzie,</font color> 2023-present; (expected
    grad. 2025) 
    </li>

    <li> <font color = #E9DCC9>Hugo Latourelle-Vigeant,</font color> 2022-2024; (PhD, Yale starting Fall
    2024); <a href=https://hugol-v.github.io/> Website: https://hugol-v.github.io/</a>
    </li>

    <li> <font color = #E9DCC9>Andrew N. Cheng,</font color>
    2021-2023; (PhD, Harvard); <a href=https://scholar.google.com/citations?user=_wfbeTsAAAAJ&hl=en>
    Google Scholar</a>
    </li>
  </ul>
  
  <h3>PhD Students</h3>
  <ul>
    <li>  <font color = #E9DCC9>Begoña García Malaxechebarría,</font
  color> 2023-present; <a href=https://begogar99.github.io/> Website: https://begogar99.github.io/</a>
    </li>
    
    <li>  <font color = #E9DCC9>Kiwon Lee,</font color> 2020-2023; (Lecturer at McGill U.)
    </li>
  </ul>

  <h3>Post Docs</h3>
  <ul>
    <li>  <font color = #E9DCC9>Inbar Seroussi,</font color> 2024;
  (assistant prof. at Tel Aviv U.); <a href=https://sites.google.com/view/inbar-seroussi/home> Website: https://sites.google.com/view/inbar-seroussi/home</a>
    </li>
    <li>  <font color = #E9DCC9>Elizabeth Collins-Woodfin,</font color> 2022-present;
  (assistant prof. at Oregon starting 2025); <a
  href=https://sites.google.com/view/e-collins-woodfin> Website: https://sites.google.com/view/e-collins-woodfin</a>
    </li>
    <li>  <font color = #E9DCC9>Yakov Vaisbourd,</font color> 2020-2022;
    </li>
  </ul>

  <h3>Google Interns</h3>
  <ul>
    <li><font color = #E9DCC9>Tomás González,</font color> 2022-2023; (PhD, Carnegie Mellon); <a href=https://scholar.google.ca/citations?user=7tDv1j4AAAAJ&hl=en> Google Scholar</a>
    </li>

    <li>  <font color = #E9DCC9>Konstantin Mishchenko,</font color>
    2021; (research scientist at Samsung AI); <a href=https://www.konstmish.com/> Website: https://www.konstmish.com/</a>
    </li>
  </ul>

</div>
</div>
</section>
					
<!-- Section --> <section class="wrapper style1" id="presentations">
<div class="inner"> <!-- 2 Columns-->
<div class="flex flex-2">
<div class="col col1">
<div class="image round fit">
  <img src="images/C_Paquette02.jpg" alt ="" /></a>
</div>
</div>
<div class="col col2">
  <h2>PRESENTATIONS</h2>
  <p>
      I have given talks on the research above at the following conferences.

  <h3>COLLOQUIUM/PLENARY SPEAKER</h3>
  <ul>
        <li>
    <i>Math Department Colloquium </i>, <b
  style="color:#2E8BC0;">University of Washington,</b> Seattle, WA, November 2023
    </li>
    
    <li>
    <i>Math Department Colloquium </i>, <b
  style="color:#2E8BC0;">Rensselaer Polytechnic Institute,</b> Troy, NY, January 2023
    </li>
    
    <li>
    Plenary speaker, <i>Conference on the Mathematical Theory of Deep
  Neural Networks </i>, <b style="color:#2E8BC0;">Deep Math,</b> UC
    San Diego, CA, November 2022
    </li>

    <li> <i>Information Systems Laboratory Colloquium,</i> <b
    style="color:#2E8BC0;">Stanford University,</b>
    October 2022
    </li> 
    
    <li> Plenary speaker, <i>GroundedML Workshop</i> <b
    style="color:#2E8BC0;">10th International Conference on Learning
    Representations (ICLR),</b>
    (virtual event), April 2022
    </li> 

    <li> <i>Courant Institute of Mathematical
    Sciences Colloquium,</i> <b
    style="color:#2E8BC0;"> New York University (NYU),</b>
    New York, NY (virtual event), January 2022
    </li>

    <li> <i>Mathematics Department Colloquium,</i> <b
    style="color:#2E8BC0;"> University of California-Davis,</b>
    Davis, CA (virtual event), January 2022
    </li>

    <li> <i>Operations Research and Financial Engineering Colloquium,</i> <b
    style="color:#2E8BC0;"> Princeton University,</b>
    Princeton, NJ (virtual event), January 2022
    </li>

    <li> <i>Computational and Applied Mathematics (CAAM) Colloquium,</i> <b
    style="color:#2E8BC0;"> Rice University,</b>
    Houston, TX, December 2021
    </li>

    <li>Plenary speaker, <i>Beyond first-order methods in machine
    learning systems Workshop,</i> <b style="color:#2E8BC0;"> International Conference on Machine Learning (ICML),</b>
    (virtual event) July 2021
    </li>

    <li><i>Operations Research Center Seminar,</i> <b
    style="color:#2E8BC0;"> Sloan School of Management, Massachusetts Institute of Technology (MIT),</b>
    Boston, MA (virtual event) February 2021
    </li>

    <li><i>Operations Research and Information Engineering (ORIE) Colloquium,</i> <b
    style="color:#2E8BC0;"> Cornell University, </b>
    Ithaca, NY (virtual event) February 2021
    </li>

    <li><i>Tutte Colloquium,</i> <b
    style="color:#2E8BC0;"> Combinatorics and Optimization Department,
    University of Waterloo, </b>
    Waterloo, ON (virtual event) June 2020
    </li>

    <li><i>Center for Artificial Intelligence Design (CAIDA) colloquium,</i> <b
    style="color:#2E8BC0;"> University of British Columbia, </b>
    Vancouver, BC (virtual event) June 2020
    </li>

    <li><i>Math Department Colloquium,</i> <b
    style="color:#2E8BC0;"> Ohio State University, </b>
    Columbus, OH, February 2019
    </li>

    <li><i>Applied Math Colloquium,</i> <b
    style="color:#2E8BC0;"> Brown University, </b>
    Providence, RI, February 2019
    </li>

    <li><i>Mathematics and Statistics Colloquium,</i> <b
    style="color:#2E8BC0;"> St. Louis University, </b>
    St. Louis, MO, November 2019
    </li>
</ul>

<h3>INVITED TALKS</h3>

<ul>
  <li> <i>Invited speaker, </i> <b
  style="color:#2E8BC0;">Portuguese American Optimization Workshop
  (PAOW), </b> Azores, June 2025 (upcoming)
  </li>
  
  <li> <i>Invited speaker, </i> <b
  style="color:#2E8BC0;">Physics of AI Algorithms, </b> Les Houches, France, January 2025 (upcoming)
  </li>
  
  <li> <i>Invited speaker, </i> <b
  style="color:#2E8BC0;">Computational Harmonic Analysis in Data Science and Machine
  Learning Workshop, </b> Oaxaca, Mexico, September 2024 (upcoming)
  </li>
  
  <li> <i>Parameter-Free Optimization (invited speaker), </i> <b
  style="color:#2E8BC0;"> 25th International Symposium on Mathematical
  Programming (ISMP), </b> Montreal, QC, July 2024 (upcoming)
  </li>
  
  <li> <i>Continuous optimization lecture, </i> <b
  style="color:#2E8BC0;"> CIFAR Deep Learning + Reinforcement Learning
  Summer School, </b> Toronto, ON, July 2024 (upcoming)
  </li>
  
  <li> <i>DIMACS Workshop on Modeling Randomness in Neural Network Training, </i> <b
  style="color:#2E8BC0;"> Rutgers University, </b> New Brunswick, NJ,
  June 2024 (upcoming)
  </li>

  <li> <i>Invited speaker, </i> <b
  style="color:#2E8BC0;"> CIFAR AI CAN, </b> Banff, AB, May 2024
  </li>
  
  <li> <i>Youth in High-dimensions, </i> <b
  style="color:#2E8BC0;"> The Abdus Salam International Centre for
  Theoretical Physics, </b> Trieste, Italy, May 2024
  </li>
  
  <li> <i>Harvard Probability Seminar Series, </i> <b
  style="color:#2E8BC0;"> Harvard University, </b> Cambridge, MA, April 2024
  </li>
  
  <li> <i>Math Machine Learning Seminar, </i> <b
  style="color:#2E8BC0;"> UCLA, </b> (virtual) 
  March 2024
  </li>
  
  <li> <i>INTER-MATH-AI Monthly Seminar, </i> <b
  style="color:#2E8BC0;"> U. of Ottawa, </b> Ottawa, ON, 
  February 2024
  </li>

  <li> <i>The Mathematics of Data Science, </i> <b
  style="color:#2E8BC0;"> Institute for Mathematical Sciences (IMS), </b> Singapore,
  January 2024
  </li>
  
  <li> <i>Optimization and Algorithm Design Workshop, </i> <b
  style="color:#2E8BC0;"> Simons Institute for the Theory of Computing, </b> Berkeley, CA,
  December 2023
  </li>
  
  <li> <i>Midwest Optimization Meeting, </i> <b
  style="color:#2E8BC0;"> University of Michigan, </b> Ann Arbor, MI,
  November 2023
  </li>
  
  <li> <i>MTL-OPT Seminar, </i> <b
  style="color:#2E8BC0;"> MILA, </b> Montreal, QC,
  October 2023
  </li>
  
  <li> <i>Invited speaker, </i> <b
  style="color:#2E8BC0;"> CodEx Seminar, </b> (virtual)
  October 2023
  </li>
  
  <li> <i>Optimization Seminar, </i> <b
  style="color:#2E8BC0;"> University of Pennsylvania, </b>
  Philadelphia, PA, September 2023
  </li>
  
  <li> <i>Invited speaker, </i> <b
  style="color:#2E8BC0;"> Foundations of Computational Mathematics, </b>
  Paris, France, June 2023
  </li>
  
  <li> <i>Invited speaker, </i> <b
  style="color:#2E8BC0;"> SIAM Conference on Optimization, </b>
  Seattle, WA, June 2023
  </li>
  
  <li> <i>DACO Seminar, </i> <b
  style="color:#2E8BC0;"> ETH Zurich, </b> Zurich, Switzerland, April 2023
  </li>
  
  <li> <i>Invited speaker, </i> <b
  style="color:#2E8BC0;"> US-Mexico Workshop on Optimization and Its Applications (In
  honor of Steve Wright’s 60th Birthday Conference), </b>
  Oaxaca, Mexico, January 2023
  </li>
  
  <li> <i>Department of Decision Sciences Seminar, </i> <b
  style="color:#2E8BC0;"> HEC, </b> Montreal, QC, December 2022
  </li>

  <li> <i>Dynamical Systems Seminar</i>, <b style="color:#2E8BC0;"> Brown
  University, </b> Providence, RI, October 2022
  </li>

  <li> <i>Tea Talk</i>, <b style="color:#2E8BC0;"> Quebec Artificial
  Intelligence Institute (MILA), </b> Montreal, QC, September 2022
  </li>
  
  <li><i>Adrian Lewis’ 60th Birthday Conference (contributed talk),</i> <b
  style="color:#2E8BC0;"> University of Washington,</b> Seattle, WA,
  August 2022
  </li>

  <li>
  <i>Stochastic Optimization Session (contributed talk),</i> <b
  style="color:#2E8BC0;"> International Conference on Continuous
  Optimization (ICCOPT 2022),</b>
  Lehigh University, Bethlehem, PA, July 2022
  </li>

  <li><i>Conference on random matrix theory and numerical linear algebra (contributed talk),</i> <b
  style="color:#2E8BC0;"> University of Washington,</b> Seattle, WA,
  June 2022
  </li>

  <li><i>Dynamics of Learning and Optimization in Brains and Machines,</i> <b
  style="color:#2E8BC0;"> UNIQUE Student Symposium,</b> MILA,
  Montreal, QC, June 2022
  </li>

  <li><i>The Mathematics of Machine Learning,</i> <b style="color:#2E8BC0;">
  Women and Mathematics,</b> Institute of Advanced Study, Princeton,
  NJ, May 2022
  </li>

  <li><i>Robustness and Resilience in Stochastic Optimization and
  Statistical Learning: Mathematical Foundations,</i> <b
  style="color:#2E8BC0;"> Ettore Majorana Foundation and Centre for
  Scientific Culture,</b> Erice, Italy, May 2022
  </li>

  <li><i>Optimization in Data Science (contributed talk),</i> <b style="color:#2E8BC0;">
  INFORMS Optimization Society Meeting 2022,</b> Greenville, SC, March
  2022
  </li>

  <li> <i>Optimization and ML Workshop (contributed talk),</i> <b
  style="color:#2E8BC0;"> Canadian Mathematical Society (CMS),
  </b>Montreal, QC, December 2021 </li>

  <li><i>Operations Research/Optimization Seminar</i>, <b style="color:#2E8BC0;">
  UBC-Okanagan and Simon Fraser University,</b> Burnaby, BC, December 2021
  </li>

  <li><i>Machine Learning Advances and Applications Seminar,</i> <b
  style="color:#2E8BC0;"> Fields Institute for Research in
  Mathematical Sciences,</b> Toronto, ON, November 2021
  </li>

  <li><i>Methods for Large-Scale, Nonlinear Stochastic Optimization Session (contributed talk),</i> <b
  style="color:#2E8BC0;"> SIAM Conference on Optimization,</b>
  Spokane, WA, July 2021
  </li>

  <li><i>MILA TechAide AI Conference (invited talk),</i> Montreal, QC,
  May 2021
  </li>

  <li><i>Minisymposium on Random matrices and numerical linear algebra (contributed talk),</i> <b
  style="color:#2E8BC0;"> SIAM Conference on Applied Linear
  Algebra,</b>, virtual event, May 2021
  </li>
  
  <li><i>Numerical Analysis Seminar (invited talk), Applied Mathematics,</i>
  <b style="color:#2E8BC0;"> University of Washington,</b> Seattle, WA,
  April 2021
  </li>

  <li><i>Applied Mathematics Seminar (invited talk), Applied Mathematics,</i> <b
  style="color:#2E8BC0;"> McGill University,</b> Montreal, QC, January
  2021
  </li>

  <li><i>Optimization and ML Workshop (contributed talk),</i> <b
  style="color:#2E8BC0;"> Canadian Mathematical Society (CMS), </b>
  Montreal, QC, December 2020
  </li>

  <li><i>UW Machine Learning Seminar (invited talk), Paul G. Allen School of Computer Science,</i> <b
  style="color:#2E8BC0;"> University of Washington,</b> Seattle, WA,
  November 2020
  </li>

  <li><i>Soup and Science (contributed talk),</i> <b style="color:#2E8BC0;">
  McGill University,</b> Montreal, QC, September 2020
  </li>

  <li> <i>Conference on Optimization,</i> <b style="color:#2E8BC0;">
  Fields Institute for Research in Mathematical Science,</b> Toronto,
  ON, November 2019
  </li>

  <li> <i>Applied Math Seminar,</i> <b style="color:#2E8BC0;">
  McGill University,</b> Montreal, QC, February 2019
  </li>

  <li> <i>Applied Math and Analysis Seminar,</i> <b style="color:#2E8BC0;">
  Duke University,</b> Durham, NC, January 2019
  </li>

  <li><i>Google Brain Tea Talk,</i> <b style="color:#2E8BC0;"> Google, </b>
  Montreal, QC, January 2019
  </li>

  <li> <i>Young Researcher Workshop, Operations Research and Information Engineering (ORIE),</i> <b
  style="color:#2E8BC0;"> Cornell University,</b> Ithaca, NY, October 2018
  </li>

  <li><i>DIMACS/NSF-TRIPODS conference,</i> <b style="color:#2E8BC0;">
  Lehigh University,</b> Bethlehem, PA, July 2018
  </li>

  <li><i>Session talk,</i><b style="color:#2E8BC0;"> INFORMS annual meeting, </b>
  Houston, TX, October 2017
  </li>

  <li> <i>Optimization Seminar,</i> <b style="color:#2E8BC0;">
  Lehigh University, </b> Bethlehem, PA, September 2017
  </li>

  <li><i>Session talk,</i> <b style="color:#2E8BC0;"> SIAM-optimization,
  </b> Vancouver, BC, May 2017
  </li>

  <li> <i> Optimization and Statistical Learning,</i>
  Les Houches, April 2017
  </li>

  <li> <i>West Coast Optimization Meeting,</i> <b style="color:#2E8BC0;">
  University of British Columbia (UBC),</b> Vancouver, BC, September
  2016 </li>
  
</ul>
</p>

<h3>SUMMER SCHOOLS & TUTORIALS</h3>
<p>
<ul>
  <li> <i>High-dimensional Learning Dynamics with Applications to
  Random Matrices,</i> <u> Random Matrices and Scaling Limits Program,</u>  <b
  style="color:#2E8BC0;"> Mittag Leffler,</b> Stockholm, Sweden,
  October 2024 (upcoming)
  </li>
  
  <li> <i>Nonconvex and Nonsmooth Optimization Tutorial,</i> <u>East Coast Optimization Meeting,</u>  <b
  style="color:#2E8BC0;"> George Mason University,</b> Fairfax, VA,
  April 2022
  </li>

  <li> <i>Average Case Complexity Tutorial,</i> <u>Workshop on Optimization under Uncertainty,</u>  <b
  style="color:#2E8BC0;"> Centre de recherches mathematiques
  (CRM),</b> Montreal, QC, September 2021
  </li>

  <li> <i>Stochastic Optimization,</i>  <b style="color:#2E8BC0;">
  Summer School talk for University of Washington’s ADSI Summer School
  on Foundations of Data Science,</b> Seattle, WA, August 2019
  </li>
</ul>
</p>
								  
</div>
</div>
</div>
</section>


<!-- Section --> <section class="wrapper style2" id="workshops">
<div class="inner" >
<div class="flex flex-2">
<div class="col col2">
  <h2>WORKSHOPS & TUTORIALS</h2>
  <h3>Workshops</h3>
  I have had the pleasure to organize some wonderful optimization
  workshops. Please consider submitting papers to
  these great organizations.

  <p></p>
  
  <ul>
    <li> <i style="color:#E9DCC9;"> <u> Optimization for Machine
    Learning Workshop part of NeurIPS </u> </i>
    </li>
    • Program Chair (2020,2021,2022,2023) • Annual event in early
    December, late November • <a href=https://opt-ml.org/> Website: https://opt-ml.org/
    </a> • Accepts papers starting in July (see website for
    details)
    <p></p>

    <li> <i style="color:#E9DCC9;"> <u> High-dimensional Learning
    Dynamics (HiLD) Workshop, NeurIPS </u> </i>
    </li>
    • Program Chair (2023,2024) • Held at ICML in mid-July • <a href=https://sites.google.com/view/hidimlearning/home>
    Website: see here
    </a> • Accepts papers starting in March (see website for details)
    <p></p>

    <li> <i style="color:#E9DCC9;"> <u> Montreal AI Symposium </u>
    </i><br>
    • Program Chair (2021) • Annual event in early September-October •
    <a href=http://montrealaisymposium.com/> Website: http://montrealaisymposium.com/
    </a> • Accepts papers starting in June (see website for
    details); Must be connected to the greater Montreal area
    </li>
  </ul>

  <h3>Tutorials</h3>
  I have organized the following tutorials based on my research. For
  more information, please see the corresponding website.
  <p> </p>
  <ul>
    <li> <i style="color:#E9DCC9;"> <u> Random Matrix Theory and Machine Learning Tutorial as part
    of ICML </u> </i> </br> • 2021 • <a
  href=https://random-matrix-learning.github.io/> Website:https://random-matrix-learning.github.io/
    </a> • 3 hour introductory tutorial on applying random matrix theory techniques in
    machine learning
    </li>
  </ul>
				      
</div>
<div class="col col1 first">
  <center>
  Eigenvalues of Wishart matrices
  </center>
  <br>

<div class="image round fit">
  <img src="images/mp_eigenvalues.jpg" alt=""/></a>
</div>
<div class="image round fit">
  <img src="images/average_worst.png" alt="" /></a>
</div>

<center>
Average-case analysis
</center>
<br>

<div class="image round fit">
  <img src="images/average_case_sub.png" alt="" /></a>
</div>

</div>
</div>
</div>
</section>


<!-- Section --> <section class="wrapper style1" id="teaching">
<div class="inner"> <!-- 2 Columns -->
<div class="flex flex-2">
<div class="col col1">
<div class="image round fit">
  <img src="images/banner.jpg" alt ="" /></a>
</div>
</div>
<div class="col col2">
  <h2>TEACHING</h2>
 
  <p> <h3>Past Courses</h3> <p>

  I have taught the following courses:

  <ul> <b> McGill University, Mathematics and Statistics Department </b>
    <li> Math 560 (graduate, instructor): Numerical Optimization,
    Winter 2021, Winter 2022
    </li>

    <li> Math 315 (undergraduate, instructor): Ordinary Differential
    Equations, Fall 2020, Fall 2021
    </li>

    <li> Math 597 (graduate, instructor):
    Topics course on Convex Analysis and Optimization,
    Fall 2021
    </li>

    <li> Math 417/517 (advanced undergraduate/graduate, instructor):
    Linear Programming,
    Fall 2022
    </li>

    <li> Math 463/563 (advanced undergraduate/graduate, instructor):
    Convex Optimization,
    Winter 2023, Winter 2024
    </li>
  </ul>

  <ul> <b> Lehigh University, Industrial and Systems Engineering </b>
    <li> ISE 417 (graduate, instructor): Nonlinear Optimization, Spring 2018
    </li>
  </ul>

  <ul> <b> University of Washington, Mathematics Department </b>
    <li> Math 125 BC/BD (undergraduate, TA): Calculus II Quiz Section, Winter 2017
    </li>

    <li> Math 307 E (undergraduate, instructor): Intro to Differential
    Equations, Winter 2016
    </li>

    <li> Math 124 CC (undergraduate, TA): Calculus 1, Autumn 2015
    </li>

    <li> Math 307 I (undergraduate, instructor): Intro to
    Differential Equations, Spring 2015</li>

    <li> Math 125 BA/BC (undergraduate, TA): Calculus
    2, Winter 2015 </li>

    <li> Math 307 K (undergraduate, instructor): Intro to Differential Equations, Autumn
    2014 </li>

    <li> Math 307 L (undergraduate, instructor): Intro to Differential
    Equations, Spring 2014 </li>
  </ul>
  </p>
</div>
</div>
</div>

</section>
<!-- Section --> <section class="wrapper style2" id="bio">
<div class="inner"> <!-- 2 Columns-->
<div class="flex flex-2">
<div class="col col1">
<div class="image fit">
  <img src="images/paquette_photo_1.jpg" alt ="" /></a>
</div>
</div>

<div class="col col2">
  <h2>Biosketch (for talks)</h2>
  <p> Courtney Paquette is an assistant professor at McGill
  University and a CIFAR Canada AI chair, MILA. She was awarded a
  Sloan Fellowship in Computer Science in 2024. Paquette’s
  research broadly focuses on designing and analyzing algorithms for
  large-scale optimization problems, motivated by applications
  in data science. She is also interested in scaling limits of
  stochastic algorithms. She received her PhD from the mathematics
  department at the University of Washington (2017), held postdoctoral
  positions at Lehigh University (2017-2018) and
  University of Waterloo (NSF postdoctoral fellowship, 2018-2019),
  and works 20% as a research scientist at Google
  DeepMind, Montreal.
  </p>

  <p> Research currently supported by CIFAR AI Chair, MILA; NSERC
  Discovery Grant; FRQNT New university researcher’s start-up program;
  Sloan Fellowship.
  </p>
</div>
</div>
</div>

</section>
<!-- Section --> <section class="wrapper style1"
id="RMT_OPT_ML_seminar">
<div class="inner"> <!-- 2 Columns-->
<div class="flex flex-2">
<div class="col col1">
<div class="image fit">
  <img src="images/RMT_OPT_ML_Seminar.png" alt ="" /></a>
</div>

<div class="image fit">
  <img src="images/Maple_leaf_neural_network.png" alt ="" /></a>
</div>
</div>
<div class="col col2">
  <h3>McGill University: <br> Random Matrix Theory & Machine
  Learning & Optimization Graduate Seminar (RMT+ML+OPT
  Seminar)</h3>

  <h3>Current Information, Fall 2024 TBD </h3>

  <p>
  All are welcome to attend (in person) at McGill University.
  <br>
  For a complete schedule, see Website
  <ul>
    <li> Website: <a
  href="https://elliotpaquette.github.io/rmtmloptseminar.html"> see here</a>
    </li>
    <li> WHEN: TBD
    </li>
    <li> WHERE: TBD
    </li>
  </ul>
  The goal of the seminar is to give graduate
  and undergraduate students the opportunity to learn how to present
  technical papers in machine learning, random matrix theory, and optimization. 
  </p>
</div>
</div>
</div>

</section>

<!-- Footer --> <footer id="footer"> <div
			class="copyright"> <p>&copy; Untitled. All
			rights reserved. Design: <a
			href="https://templated.co">TEMPLATED</a>. </p>
			</div> </footer>

		<!-- Scripts --> <script
			src="assets/js/jquery.min.js"></script>
			<script
			src="assets/js/jquery.scrolly.min.js"></script>
			<script
			src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body> </html>
