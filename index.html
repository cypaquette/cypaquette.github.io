<!DOCTYPE HTML>
<!--
	Urban by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
	<head>
		<title>Courtney Paquette</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body>


		<!-- Banner -->
			<section id="banner">
				<div class="inner">
					<header>
						<h1>Courtney Paquette</h1>
				  <p>
			Assistant Professor, Mathematics
				  and Statistics, McGill
			University</p>
				  </header>
				  <a href="#aboutme" class="button
			big scrolly">About Me</a> &nbsp; &nbsp;	&nbsp;
			&nbsp; &nbsp; <a href="#research" class="button
			big scrolly">Research</a> &nbsp; &nbsp;	&nbsp;
			&nbsp; &nbsp; <a href="#teaching"
			class="button big scrolly">Teaching</a>  &nbsp; &nbsp;	&nbsp;
			&nbsp; &nbsp; <a href="Research/CV.pdf" class="button
			big scrolly">CV</a>
				</div>
			</section>

		<!-- Main -->
			<div id="main">

				<!-- Section -->
					<section class="wrapper
					style1" id="aboutme">
						<div class="inner">
							<!-- 2 Columns -->
								<div class="flex flex-2">
									<div class="col col1">
								<div class="image round fit">
							<img
					src="images/paquette_photo_1.jpg"
					alt ="" /></a>
										</div>
									</div>
									<div
					class="col col2">
									  <h3>HELLO!</h3>
									  <p>I
					am an assistant professor at
					McGill University in the
					 <a
					href=https://www.mcgill.ca/mathstat/><u>Mathematics and Statistics
					department</u> </a>. I am
					a CIFAR AI Chair and I am an active
					member of the Montreal Machine
					Learning
					Optimization Group <a href=https://mtl-mlopt.github.io/><u>(MTL MLOpt)</u></a> at <a
					href=https://mila.quebec/en/><u>MILA.</u></a>
					Moreover I am the lead
					organizer of the <a href=https://opt-ml.org/><u>OPT-ML
					Workshop</u></a> for NeurIPS
					2020. Previously, I was a research
					scientist at <a
					href=https://ai.google/research/teams/brain/><u>Google
					Brain, Montreal.</u></a> You can view my CV <a
					href="Research/CV.pdf"><u>here</u></a> if you are
					interested in more details.
					</p>
										<p>I
					received my Ph.D. from the
					Mathematics department at the University
					of Washington (2017) under <a href=http://www.math.washington.edu/~ddrusv/><u>Prof. Dmitriy
									  Drusvyatskiy</u></a> then I
					held a postdoctoral
					position in the
					Industrial and Systems
					Engineering at Lehigh
					University where I worked with <a
					href=https://coral.ise.lehigh.edu/katyas/><u>Prof. Katya Scheinberg</u></a>. I
					held an NSF postdoctoral
					fellowship (2018-2019) under
					<a href=https://uwaterloo.ca/scholar/vavasis><u>Prof. Stephen
					Vavasis</u></a> in the
					Combinatorics and Optimization
					Department at the University
									  of
									  Waterloo.</p>
									  
									  <!--Currently,
					I am Ross Assistant Professor
					(Post Doctoral position) at
					Ohio State
					University. Starting in
					January 2018, I will be a post
					doc in the Industrial and
					Systems Engineering Department
					of Lehigh University. I will
					be co-advised by <a
					href=https://coral.ise.lehigh.edu/katyas/><u>Prof. Katya Scheinberg
					</u></a>, <a
					href=https://coral.ise.lehigh.edu/frankecurtis/><u>Prof. Frank E.
					Curtis
					</u></a>, and <a
					href=http://mtakac.com/><u>Prof. Martin
					Tak&aacute&#x10d
									  </u></a>. </p> -->

									  
					<p> My research broadly
					focuses on designing and
					analyzing algorithms for
					large-scale optimization
					problems, motivated by
					applications in data
					science. The techniques I use
					draw from a variety of fields
					including probability,
					complexity theory, and convex
					and nonsmooth analysis. </p> <!--I study optimization, in
					particular continuous
					optimization and first order
					methods. My Ph.D advisor at
					the University of Washington was <a href=http://www.math.washington.edu/~ddrusv/><u>Prof. Dmitriy
									  Drusvyatskiy</u></a>.-->

									 <p>
					University of Washington, Lehigh University,
					University of Waterloo, McGill
					University, and MIlA  have
					strong optimization groups
					which spans across many
					departments: Math,
					Stats, CSE, EE, and ISE. If you are
					interested in optimization
					talks at these places,
					check out the following seminars:
									  <ul><li> <a
					href=https://opt-ml.org/><u>
					Optimization for Machine Learning (OPT+ML) </u></a>
					workshop at NeurIPS
									    </li>
									    <li> <a
					href=https://mtl-mlopt.github.io/><u>
					Montreal Machine Learning and
					Optimization (MTL MLOPT) </u></a>
					at MILA
									    </li>
									    <li> <a
					href=https://dms.umontreal.ca/~mathapp/><u>
					Applied Mathematics </u></a>
					at McGill University
									    </li>
									    <li> <a
					href=http://blogs.uw.edu/tops/><u>Trends
					in Optimization Seminar (TOPS/CORE) </u></a>
					at University of Washington
									    </li>
									    <li>
					<a
					href=http://ads-institute.uw.edu/IFDS/about.html><u>Institute
					for Foundations of Data Science</u></a>
					at University of
					Washington/University of Wisconsin
									    </li>
									    <li>
					<a
					href=http://ads-institute.uw.edu/IFDS/about.html><u>Machine
					Learning</u></a>
					at Paul G. Allen School of
					Computer Science and
					Engineering, University of
					Washington
									    </li>
									    <li> <a
					href=https://coral.ise.lehigh.edu/><u>
					COR&#64L
									    </u></a>
					at Lehigh University</li>
									    <li> <a
					href=https://uwaterloo.ca/combinatorics-and-optimization/><u>
					Combinatorics and Optimization
									    
									    </u></a>
					at University of Waterloo
					<ul> <li> <a
					href="Research/U_Waterloo_Seminar_ML.pdf"><u>Machine Learning Notes from
					Fall Term 2018</u></a></li>
					<li> <a
					href="Research/U_Waterloo_Seminar_HDP.pdf"><u>High Dimensional
					Probability Notes from Spring
					Term 2019</u></a> </li>

									    </li> </ul>
									    </ul>
									  </p>
									  <!--<p>
					Exciting news! I recently got
					married to another
					mathematician, <a
					href="https://people.math.osu.edu/paquette.30/">
									  <u>Elliot Paquette.</u></a></p>-->
									  <p>
									  EMAIL:
					yumiko88(at)uw(dot)edu or yumiko88(at)u(dot)washington(dot)edu
									    or
									    courtney(dot)paquette(at)mcgill(dot)ca
									    </p>
									    <p>
					     OFFICE: BURN 913
									  </p>
									</div>
								</div>
						</div>
					</section>

				<!-- Section -->
					<section class="wrapper
					style2" id="research">
						<div class="inner" 
				 	> 
							<div class="flex flex-2">
								<div
					class="col col2">
									<h3>RESEARCH</h3>
									<p>My research interests lie at the frontier of large-scale continuous optimization.  Nonconvexity, nonsmooth analysis, complexity bounds, and interactions with random matrix theory and high-dimensional statistics appear throughout work. Modern applications of machine learning demand these advanced tools and motivate me to develop theoretical guarantees with an eye towards immediate practical value. My current research program is concerned with developing a coherent mathematical framework for analyzing average-case (typical) complexity and exact dynamics of learning algorithms in the high-dimensional setting.
								  </p>
								  <p>
					You can view my CV <a
					href="Research/CV.pdf"><u>here</u></a> if you are
					interested in more details.
								  </p>

								  <p>
								  You
								  can
								  view
								  my
								  thesis
								  titled:  
								  <a
					href="Research/CP_thesis_v2.pdf"><u>Structure
								  and
								  complexity
								  in
								  non-convex
								  and
								  nonsmooth
								  optimization. </u></a> 
								  </p>
								  <h3>PAPERS
								 </h3>
								  <p>
								   *
								  student author
								  <ul>
<li>C. Paquette, E. Paquette, B. Adlam, J. Pennington <b> <i
    style="color:#D3D3D3;">
Implicit Regularization or Implicit Conditioning? Exact Risk Trajectories of SGD in High Dimensions. </i></b>
								  (submitted), 2022,
    <a
    href=><u>
    arXiv pdf</u></a>
    
    </li>
    <p>
    </p>
    <li>K. Lee*, A.N. Cheng*, E.Paquette, C. Paquette.<b> <i
    style="color:#D3D3D3;">
Trajectory of Mini-Batch Momentum: Batch Size Saturation and Convergence in High-Dimensions. </i></b>
								  (submitted), 2022,
    <a
    href=https://arxiv.org/pdf/2206.01029.pdf><u>
    arXiv pdf</u></a>
    
    </li>
    <p>
    </p>
								    <li>C. Paquette, E. Paquette, B. Adlam, J. Pennington <b> <i
    style="color:#D3D3D3;">
 Homogenization of SGD in high-dimensions: Exact dynamics and
								  generalization
								  properties. </i></b>
								  (submitted), 2022,
    <a
    href=https://arxiv.org/pdf/2205.07069.pdf><u>
    arXiv pdf</u></a>
    
    </li>
    <p>
    </p>
								    <li>L. Cunha*, G. Gidal, F. Pedregosa, C. Paquette, D.Scieur. <b> <i
    style="color:#D3D3D3;">
   Only Tails Matter: Average-case Universality and Robustness in the
								  Convex
								  Regime. </i></b>
								  accepted
								  ICML, 2022,
    <a
    href=><u>
    pdf</u></a>
    
    </li>
    <p>
    </p>

<li>C. Paquette and E. Paquette. <b> <i
    style="color:#D3D3D3;">
    Dynamics of Stochastic Momentum Methods on Large-scale, Quadratic Models. </i></b> Advances in Neural Information Processing Systems (NeurIPS), volume 34, 2021,
    <a
    href=https://proceedings.neurips.cc/paper/2021/file/4cf0ed8641cfcbbf46784e620a0316fb-Paper.pdf><u>
    pdf</u></a>
    
    </li>
    <p>
    </p>
    <li>C. Paquette, K. Lee*, F. Pedregosa, and E. Paquette. <b> <i
								  style="color:#D3D3D3;">
								  SGD
								  in
								  the
								  Large:
								  Average-case
								  Analysis,
								  Asymptotics,
								  and
								  Stepsize
								  Criticality.</i> </b>
        Proceedings of Thirty Fourth Conference on Learning Theory (COLT) (2021) no. 134, 3548--3626,
								  <a
                                      http://proceedings.mlr.press/v134/paquette21a/paquette21a.pdf><u>
								  pdf</u></a>
								  
								  </li>
								  <p>
								  </p>
								  
<li>C. Paquette, B. van Merrienboer, F. Pedregosa, and
  E. Paquette.  <b> <i style="color:#D3D3D3;"> Halting time is predictable for large models: A
  Universality Property and Average-case Analysis.</i> </b> (2020) (to
  appear in <i>Found. Comput. Math.</i>), <a
					href=https://arxiv.org/abs/2006.04299><u>arXiv
								    pdf</u></a>
								    
								  </li>
								  <p>
								  </p>
								  <li>S. Baghal, C. Paquette, and SA Vavasis.  <b> <i style="color:#D3D3D3;">A termination criterion for stochastic gradient for binary classification.</i> </b> (2020) (submitted), <a
					href=https://arxiv.org/abs/2003.10312><u>arXiv
								    pdf</u></a>
								    
								  </li> <p> </p>
								    <li>
								  C. Paquette
								  and
								  S. Vavasis. <b> <i style="color:#D3D3D3;"> Potential-based
								  analyses
								  of
								  first-order
								  methods
								  for
								  constrained
								  and
								  composite
								  optimization.
								    </i>
								  </b> 
								 (2019)
								  (submitted),
								  <a
					href=https://arxiv.org/abs/1903.08497><u>
								    arXiv pdf
								    </u> </a>
								 
								    </li> <p> </p>
								    <li>
								  C. Paquette
								  and
								  K. Scheinberg. <b> <i style="color:#D3D3D3;">A stochastic line-search method with
convergence rate.</i> </b> SIAM J. Optim. (30) (2020) no. 1, 349-376,
								    <a
								    href=https://doi.org/10.1137/18M1216250>
								    doi:10.1137/18M1216250,</u></a> <a
					href=https://arxiv.org/pdf/1807.07994><u>arXiv
								    pdf</u> </a> 
								    </li> <p> </p>
								    <li>
					D. Davis, D. Drusvyatskiy,
								  K. MacPhee,
								  and
								  C. Paquette.
								  <b>
								    <i
								    style="color:#D3D3D3;">
								    Subgradient
								  methods
								  for
								  sharp
								  weakly
								  convex
								    functions. </i> </b>
								    J. Optim. Theory
								    Appl. (179)
								    (2018)
								    no. 3,
								    962-982,
								    <a
								    href
								    = https://doi.org/10.1007/s10957-018-1372-8><u>doi:10.1007/s10957-018-1372-8</u></a>,
								    <a
					href=https://arxiv.org/abs/1803.02461><u>
								    arXiv
								    pdf
								    </u> </a>
					</li> <p> </p> <li>
					D. Davis, D. Drusvyatskiy, and
								  C. Paquette. <b>
								    <i
								    style="color:#D3D3D3;"> The
								  nonsmooth
								  landscape
								  of
								  phase
								    retrieval. </i>
								    </b>
								    IMA
								    J. Numer. Anal. (40)
								    (2020)
								    no.4,
								    2652-2695,
								    <a
					href=https://doi.org/10.1093/imanum/drz031><u> doi:10.1093/imanum/drz031</u></a>, 
								    <a
					href=https://arxiv.org/abs/1711.03247><u>
								    arXiv
								    pdf
								    </u> </a>
					</li> <p> </p>
								    <li>
					C. Paquette, H. Lin,
					D. Drusvyatskiy, J. Mairal,
					and Z. Harchaoui. <b><i style="color:#D3D3D3;">Acceleration
					for Gradient-Based Non-Convex
								    Optimization.
								    </i>
								    </b>
								    22nd
								    International
Conference on Artificial Intelligence and Statistics (AISTATS 2018), <a
					href=http://proceedings.mlr.press/v84/paquette18a.html><u>
								    arXiv
								    pdf
								    </a> </u>
					</li> <p> </p>
								    <li>
					D. Drusvyatskiy and
					C. Paquette. <b><i style="color:#D3D3D3;"> Efficiency of
					minimizing compositions of
					convex functions and smooth
					maps.</b></i>
								    Math. Program. 178
								    (2019),
								    no. 1-2,
								    Ser. A,
								    503-558,
								    <a
								    href=https://doi.org/10.1007/s10107-018-1311-3><u>
								    doi:10.1007/s10107-018-1311-3</u></a>,  <a
								    href=https://arxiv.org/pdf/1605.00125.pdf><u>arXiv pdf</u></a>
								  </li> <p> </p>
 <li> D. Drusvyatskiy and C. Paquette.  <b><i style="color:#D3D3D3;">Variational analysis of
					spectral functions
					simplified.</b></i> J. Convex
								  Anal. 25(1),
								    2018.
								    <a
								    href=https://arxiv.org/pdf/1506.05170.pdf><u>
								    arXiv
								    pdf</u></a></li>
								  

								    </ul>
								    </p>

								    <h3>INVITED TALKS</h3>
								    <p>I
								    have
								    given
								    talks
								    on
								    the
								    research
								    above
								    at
								    the
								    following
								    conferences:
								    <ul>
								      <li>
								    <i>Halting Time is Predictable for Large Models: A Universality Property and Average-case Analysis,
								    </i>
								    <a
								    href=https://orc.mit.edu/seminars-events><u>
								   Operations Research Center Seminar, Sloan School of Management, Massachusetts Institute of Technology (MIT),
								    </u></a>
								    Boston,
								    MA
								    (February
								    2021,
								    upcoming)
								    <p> </p>
								      <li>
								    <i>Halting Time is Predictable for Large Models: A Universality Property and Average-case Analysis,
								    </i>
								    <a
								    href=https://www.orie.cornell.edu/orie/><u>
								    colloquium,
								    Operations
								    Research
								    and
								    Information
								    Engineering
								    (ORIE,
								    Cornell
								    University, 
								    </u></a>
								    Ithaca,
								    NY
								    (February
								    2021)
								    <p> </p>
								      <li>
								    <i>Halting Time is Predictable for Large Models: A Universality Property and Average-case Analysis,
								    </i>
								    <a
								    href=https://dms.umontreal.ca/~mathapp/><u>
								    Applied
								    Mathematics
								    Seminar,
								    McGill University
								    </u></a>
								    Montreal,
								    QC (January 2021)
								    <p> </p>
								      <li>
								    <i>Halting Time is Predictable for Large Models: A Universality Property and Average-case Analysis,
								    </i>
								    <a
								    href=https://winter20.cms.math.ca/><u>
								    Optimization
								    and
								    ML
								    Workshop,
								    Canadian Mathematical Society (CMS)</u></a>
								    Montreal,
								    QC
								    (December
								    2020)
								    <p> </p>
								      <li>
								    <i>Halting Time is Predictable for Large Models: A Universality Property and Average-case Analysis,
								    </i>
								    <a
								    href=https://www.cs.washington.edu/research/ml/seminars><u>UW
								    Machine
								    Learning
								    Seminar,</u></a>
								    Paul G. Allen School of Computer Science, University of Washington, Seattle, WA (November 2020)
									</u></a> <p></p>
								      <li>
								    <i>Halting Time is Predictable for Large Models: A Universality Property and Average-case Analysis,
								    </i>
								    <a
								    href=https://www.engineering.pitt.edu/Departments/Industrial/><u>Industrial Engineering, University of Pittsburgh,</u></a>
								    Pittsburgh, PA (November 2020)
									</u></a> <p></p>
								      <li>
								    <i>Halting Time is Predictable for Large Models: A Universality Property and Average-case Analysis,
								    </i>
								    <a
								    href=https://www.mcgill.ca/science/research/undergraduate-research/soupscience><u>Soup
								    and
								    Science,
								    McGill University,
								    </u></a>
								    Montreal, QC (September 2020)
									</u></a> <p></p>
								      <li>
								    <i>Halting Time is Predictable for Large Models: A Universality Property and Average-case Analysis,
								    </i>
								 Tutte Colloquium,
								    <a
								    href=https://uwaterloo.ca/combinatorics-and-optimization/><u>Combinatorics
								    and
								    Optimization
								    Department, University of Waterloo,</u></a>
								    Waterloo, ON (June 2020)
									</u></a> <p></p>
								      <li>
								    <i>Halting Time is Predictable for Large Models: A Universality Property and Average-case Analysis,
								    </i>
								 colloquium,
								    <a
								    href=https://caida.ubc.ca/><u>Center
								    for
								    Artificial
								    Intelligence
								    Design
								    (CAIDA),</u></a>
								    University of British Columbia, Vancouver, BC (June 2020)
									</u></a> <p></p>

								      <li>
								    <i>An adaptive line search method for stochastic optimization,
								    </i>
								  Conference
								    on
								    Optimization,
								    <a
								    href=http://www.fields.utoronto.ca/><u>Fields Institute for Research in Mathematical Science,</u></a>
								    Toronto, ON
								    (November 2019)
									</u></a> <p></p>

								     <li>
								    <i>Algorithms for stochastic nonconvex and nonsmooth optimization,
								    </i>
								    St. Louis
								    University
								    Mathematics
								    and
								    Statistics
								    Colloquium,
								    St
								    Louis,
								    MO
								    (November 2019)
									</u></a> <p></p>
									
								    <li>
								    <i>
								    Stochastic
								    Optimization:
								    summer
								    school talk
								    </i>,
								    <a
								    href="https://alecgt.github.io/adsi_summer/"><u>
								    ADSI
								    Summer
								    School
								    on
								    Foundations
								    of
								    Data Science
								    </u></a>
Seattle, WA (Aug. 2019); My
								    notes
								    can
								    be
								    found
								    <a
								    href="Research/summer_school.pdf"><u>here</u></a>
								      </li>
								    <p>
								      </p>
<li>
								    <i>
								    Algorithms
								    for
								    stochastic
								    problems
								    lacking
								    convexity
								    or smoothness
								    </i>,
								    <a
								    href="https://math.osu.edu/"><u>
								    Mathematics
								    Colloquium,
								    Ohio
								    State
								    University
								    </u></a>
Columbus, OH (Feb. 2019); My
								    slides
								    can
								    be
								    found
								    <a
								    href="Research/OSU.pdf"><u>here</u></a>
								      </li>
								    <p>
								      </p>
								      <li>
								    <i>
								    Algorithms
								    for
								    stochastic
								    problems
								    lacking
								    convexity
								    or smoothness
								    </i>,
								    <a
								    href="https://www.brown.edu/academics/applied-mathematics/"><u>
								    Applied
								    Mathematics
								    Colloquium,
								    Brown University
								    </u></a>
Providence, RI (Feb. 2019); My
								    slides
								    can
								    be
								    found
								    <a
								    href="Research/Brown.pdf"><u>here</u></a>
								      </li>
								    <p>
								      </p>
								      <li>
								    <i>
								    Algorithms
								    for
								    stochastic
								    problems
								    lacking
								    convexity
								    or smoothness
								    </i>,
								    <a
								    href="https://www.mcgill.ca/mathstat/"><u>
								    Applied
								    Mathematics
								    Seminar,
								    McGill University
								    </u></a>
Montreal, QC (Feb. 2019); My
								    slides
								    can
								    be
								    found
								    <a
								    href="Research/McGill.pdf"><u>here</u></a>
								      </li>
								    <p>
								      </p> <li>
								    <i>
								    Algorithms
								    for
								    stochastic
								    problems
								    lacking
								    convexity
								    or smoothness
								    </i>,
								    <a
								    href="https://math.duke.edu/events/applied-math-and-analysis"><u>
								    Applied
								    math
								    and
								    analysis
								    seminar,
								    Duke University
								    </u></a>
Durham, NC (Jan. 2019); My
								    slides
								    can
								    be
								    found
								    <a
								    href="Research/Duke.pdf"><u>here</u></a>
								      </li>
								    <p>
								      </p>
								    <li>
								    <i>
								    Algorithms
								    for
								    stochastic
								    problems
								    lacking
								    convexity
								    or smoothness
								    </i>,
								    <a
								    href="https://ai.google/research/teams/brain"><u>
								    Google
								    Brain, Montreal</u></a>
Montreal, QC (Jan. 2019); My
								    slides
								    can
								    be
								    found
								    <a
								    href="Research/Google_1.pdf"><u>here</u></a>
								      </li>
								    <p>
								      </p>
								      
								      <li>
								    <i>
								    An
								    adaptive
								    line
								    search
								    method
								    for
								    stochastic optimization</i>,
								    <a
								    href="https://www.orie.cornell.edu/orie"><u>Cornell
								    ORIE's
								    Young
								    Researchers Workshop
								    (2018),</u></a>
Ithaca, NY (Oct. 2018); My
								    slides
								    can
								    be
								    found
								    <a
								    href="Research/Cornell_paquette.pdf"><u>here</u></a>
								      </li>
								    <p>
								      </p>
								      <li>
								    <i>
								    New
								    analysis
								    of
								    adaptive
								    stochastic
								    optimization
								    methods
								    via
								    supermartingales
								    Part
								    II:
								    Convergence
								    analysis
								    for
								    stochastic
								    line search</i>,
								    <a
								    href="http://optml.lehigh.edu/events/dimacs-tripods-mopta-2018/"><u>
								    Lehigh
								    University DIMACS
								    (2018)</u></a>,
Bethlehem, PA (August. 2018); My
								    slides
								    can
								    be
								    found
								    <a
								    href="Research/DIMACS_18.pdf"><u>here</u></a>
								      </li> <p> </p>
								      <li>
								    <i>
								    Generic
								    Acceleration
								    Schema
								    Beyond
								    Convexity
								    </i>,
								    <a
								    href="https://www.informs.org/"><u>INFORMS
								    annual
								    meeting
								    (2017),</u></a>
Houston, TX (Oct. 2017); My
								    slides
								    can
								    be
								    found
								    <a
								    href="Research/INFORMS_2.pdf"><u>here</u></a>
								      </li> <p> </p>
								      <li>
								    <i>
								    Minimization
								    of
								    convex
								    composite,</i>
								    Lehigh
								    University
								    Optimization
								    Seminar,
Bethlehem, PA (Sept 2017); My
								    slides
								    can
								    be
								    found
								    <a
								    href="Research/Lehigh_cvx_comp.pdf"><u>here</u></a>
								      </li>
								    <p> </p>
								      <li><i>Proximal methods for minimizing convex compositions,</i> SIAM-optimization,
Vancouver, BC (May 2017) 
								      </li>
								    <p> </p>
								      <li> <i>Catalyst for Gradient-based Nonconvex Optimization</i>, Inria-Grenoble Seminar,
Grenoble (April 2017)
								      </li> <p> </p>
								      <li><i>Generic acceleration schema beyond convexity</i>, Optimization and Statistical
Learning, Les Houches (April 2017)
								      </li> <p> </p>
								      <li>
								    <i>Proximal
								    methods
								    for
								    minimizing
								    convex
								    compositions,</i>
								    <a
								    href="http://www.cs.ubc.ca/~mpf/wcom-fall-2016/#!schedule.md"><u>West
								    Coast
								    Optimization
								    Meeting</u></a>,
								    University
								    of
								    British
								    Columbia
								    (September
								    2016);
								    My
								    slides
								    can
								    be
								    found
								    <a
								    href="Research/cnx_comp.pdf"><u>here</u></a>
								    </li> </ul>
								    </p>
								  
								  
								  
								</div>
								<div class="col col1 first">
									<div class="image round fit">
										<img src="images/C_Paquette02.jpg" alt="" /></a>
									</div>
								</div>
							</div>
						</div>
					</section>

				<!-- Section -->
					<section class="wrapper
					style1" id="teaching">
						<div class="inner">
							<!-- 2 Columns -->
								<div class="flex flex-2">
									<div class="col col1">
								<div class="image round fit">
							<img
					src="images/banner.jpg"
					alt ="" /></a>
										</div> 
									</div>
									<div
					class="col col2">
									  <h3>TEACHING</h3>
									  <h3>Current
					Course
									  </h3>
									  <p> <ul>
                                      <li> Math 315 (Ordinary Differential Equations) <a
                                          href="https://mycourses2.mcgill.ca/d2l/home">
                                          Website </a> </li>
                                      <li> Math 597 (Topics course: Convex Analysis and Optimization) <a
                                          href="https://mycourses2.mcgill.ca/d2l/home">
                                          Website </a> </li> </ul>
									  </p>
										<p>
									  <h3>Past
									  Courses</h3>
									  <p>
					I have taught the following
									  courses:
                                      <ul> <b> McGill University, Mathematics and Statistics Department </b>
                                           <li>
                    Math 560 (graduate, instructor): Numerical Optimization, Spring 2021
                    </li><li>
                        Math 315 (undergraduate, instructor): Ordinary Differential Equations, Fall 2020
                        </li>
                                        </ul>
                                      <ul> <b> Lehigh University, Industrial and Systems Engineering </b>
                                        <li>
                    ISE 417 (graduate, instructor): Nonlinear Optimization, Spring 2018
                    </li>
                                        </ul>
									  <ul> <b> University of Washington, Mathematics Department </b>
									    <li>
					Math 125 BC/BD (undergraduate, TA): Calculus II
					Quiz Section, Winter 2017
					</li> 
					<li> Math 307 E (undergraduate, instructor): Intro to Differential
									    Equations, Winter 2016 </li>
									    <li>
					Math 124 CC (undergraduate, TA): Calculus 1,
					Autumn 2015 </li>
					<li> Math 307 I (undergraduate, instructor): Intro to
					Differential Equations, Spring
					2015</li>
					<li> Math 125 BA/BC (undergraduate, TA): Calculus
					2, Winter 2015 </li>
					<li> Math 307 K (undergraduate, instructor): Intro to
					Differential Equations, Autumn
					2014 </li>
					<li> Math 307 L (undergraduate, instructor): Intro to
					Differential Equations, Spring
					2014 </li>
									    </ul>
									  </p>
									</div>
								</div>
						</div>
					</section>

		<!-- Footer -->
			<footer id="footer">
				<div class="copyright">
					<p>&copy; Untitled. All rights reserved. Design: <a href="https://templated.co">TEMPLATED</a>. </p>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
